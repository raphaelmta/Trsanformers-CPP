# bumblebee â€” Transformer em C++ puro ðŸ¤–ðŸ

![Linguagem](https://img.shields.io/badge/Linguagem-C++20-blue.svg)
![LicenÃ§a](https://img.shields.io/badge/LicenÃ§a-MIT-green.svg)

> Projeto de estudo e desenvolvimento pessoal: **transformando** cafÃ© em cÃ³digo e C++ em IA. Uma arquitetura Transformer completa (LLM) implementada do zero â€” sem â€œcaronaâ€ em frameworks de machine learning! â˜•ðŸš€

---

## Sobre o Projeto

Este repositÃ³rio documenta o desenvolvimento de um Modelo de Linguagem Grande (LLM) baseado na arquitetura **Transformer**, feito 100% em C++ padrÃ£o (C++20).  
O objetivo Ã© puramente didÃ¡tico e experimental: estudar e entender a fundo o funcionamento interno dos Transformers modernos â€” como GPT ou BERT â€” ao construir cada componente essencial da arquitetura â€œna unhaâ€, em classes C++ independentes.

> **Por que â€œbumblebeeâ€?**  
> AlÃ©m do trocadilho com os robÃ´s Transformers ðŸ¦¾, a ideia Ã© mostrar que, assim como o inseto que nÃ£o deveria voar mas voa, um estudante focado pode fazer IA voar sÃ³ com C++ puro â€” sem precisar de â€œmegatronâ€ de frameworks! ðŸ

---

## O que estÃ¡ implementado

- **TokenizaÃ§Ã£o personalizada** (Tokenizer) â€” porque dividir para conquistar nunca sai de moda!
- **Embeddings** e codificaÃ§Ã£o posicional â€” o GPS dos tokens, para ninguÃ©m se perder na sequÃªncia.
- **LayerNorm** â€” alinhando as energias da rede, zen total.
- **Self-Attention** (atenÃ§Ã£o prÃ³pria) â€” aqui cada token Ã© narcisista por natureza.
- **Feed-Forward Network** â€” porque, Ã s vezes, Ã© preciso ir direto ao ponto.
- **Encoder e Decoder** (com mÃºltiplas camadas, compondo o modelo completo) â€” igual cebola, cada camada faz vocÃª chorar de alegria!
- **CÃ¡lculo da perda** (cross-entropy) â€” a dor faz parte do aprendizado, literalmente.
- **Pipeline funcional**: o executÃ¡vel principal (`bumblebee.cpp`) orquestra os mÃ³dulos e mostra como conectar tudo sem transformar em bagunÃ§a.

Cada componente da arquitetura virou uma **classe C++ separada** â€” mais modular que braÃ§o de Transformer!

---

## Estrutura do Projeto

```text
TRANSFORMERSCPP/
â”œâ”€â”€ dados/
â”‚   â”œâ”€â”€ dataset.txt
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ 01RMTAEmbedding.hpp
â”‚   â”œâ”€â”€ 02RMTATokenizer.hpp
â”‚   â”œâ”€â”€ 03RMTAPositionalEncoding.hpp
â”‚   â”œâ”€â”€ 04RMTALayerNorm.hpp
â”‚   â”œâ”€â”€ 05RMTASelfAttention.hpp
â”‚   â”œâ”€â”€ 06RMTAFeedForwardNetwork.hpp
â”‚   â”œâ”€â”€ 07RMTAEncoderLayer.hpp
â”‚   â”œâ”€â”€ 08RMTAEncoder.hpp
â”‚   â”œâ”€â”€ 09RMTADecoderLayer.hpp
â”‚   â”œâ”€â”€ 10RMTADecoder.hpp
â”‚   â”œâ”€â”€ FinalLayer.hpp
â”‚   â”œâ”€â”€ HelpFunc.hpp
â”‚   â””â”€â”€ VectorOp.hpp
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 01RMTAEmbedding.cpp
â”‚   â”œâ”€â”€ 02RMTATokenizer.cpp
â”‚   â”œâ”€â”€ 03RMTAPositionalEncoding.cpp
â”‚   â”œâ”€â”€ 04RMTALayerNorm.cpp
â”‚   â”œâ”€â”€ 05RMTASelfAttention.cpp
â”‚   â”œâ”€â”€ 06RMTAFeedForwardNetwork.cpp
â”‚   â”œâ”€â”€ 07RMTAEncoderLayer.cpp
â”‚   â”œâ”€â”€ 08RMTAEncoder.cpp
â”‚   â”œâ”€â”€ 09RMTADecoderLayer.cpp
â”‚   â”œâ”€â”€ 10RMTADecoder.cpp
â”‚   â”œâ”€â”€ FinalLayer.cpp
â”‚   â””â”€â”€ VectorOp.cpp
â”œâ”€â”€ bumblebee.cpp
â”œâ”€â”€ bumblebee
â”œâ”€â”€ LEIAME.txt
â””â”€â”€ README.md
```


O programa lÃª `dados/dataset.txt`, processa os dados e imprime os resultados do modelo no console. Se der erro, nÃ£o se desespere: Ã© sÃ³ dar um rebootâ€¦ igual no filme!

---

## Contribuindo ðŸš—ðŸ’¨

ContribuiÃ§Ãµes e sugestÃµes sÃ£o sempre bem-vindas!  
Para colaborar e entrar na â€œequipe dos Autobotsâ€:

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'feat: Adiciona NovaFuncionalidade'`)
4. DÃª push na branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request




---

> â€œA melhor forma de entender um Transformer Ã© construir um. A segunda melhor forma Ã©â€¦ bom, pelo menos tente rodar esse repositÃ³rio!â€  
> â€” O Bumblebee (provavelmente)

Projeto mantido por Raphael Martins.  
GitHub: [@raphaelmta](https://github.com/raphaelmta)  
LinkedIn: [https://www.linkedin.com/in/raphaelmta/](https://www.linkedin.com/in/raphaelmta/)